{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to BlazingSQL Notebooks!\n",
    "\n",
    "BlazingSQL Notebooks is a fully managed, high-performance Jupyter notebook environment. \n",
    "\n",
    "**No setup required.** You just login and start writing code, immediately.\n",
    "\n",
    "Every Notebooks environment has:   \n",
    "- An attached CUDA GPU\n",
    "- Pre-Installed GPU Data Science Packages ([BlazingSQL](https://github.com/BlazingDB/blazingsql), [RAPIDS](https://github.com/rapidsai), [Dask](https://github.com/dask), and many more)\n",
    "\n",
    "Start running GPU-accelerated code below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DataFrame\n",
    "The RAPIDS ecosystem is built on the concept of a shared GPU DataFrame between all of the different libraries and packages.\n",
    "\n",
    "There are two libraries specific to data manipulation:\n",
    "- **BlazingSQL**:  SQL commands on a GPU DataFrame\n",
    "- **cuDF**: Pandas-like commands on a GPU DataFrame.\n",
    "\n",
    "### BlazingSQL (BSQL) \n",
    "[GitHub](https://github.com/BlazingDB/blazingsql) | [Intro Notebook](intro_notebooks/blazingcontext.ipynb)\n",
    "\n",
    "Let's see how easy it is to SQL query a CSV file on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazingsql import BlazingContext\n",
    "\n",
    "# Create a BlazingContext to launch a BSQL session\n",
    "bc = BlazingContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create a BlazingSQL table from any file w/ .create_table(table_name, file_path)\n",
    "bc.create_table('taxi', f'{os.getcwd()}/data/sample_taxi.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query table with the `.sql()` function\n",
    "bc.sql('SELECT * FROM taxi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about [creating](intro_notebooks/create_tables.ipynb) and [querying](intro_notebooks/query_tables.ipynb) BlazingSQL tables, or the [BlazingContext API](intro_notebooks/blazingcontext.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingSQL returns each query's results as a cuDF DataFrame, making for easy handoff to GPU or non-GPU solutions.\n",
    "\n",
    "### cuDF\n",
    "[GitHub](https://github.com/rapidsai/cudf) | [Intro Notebook](intro_notebooks/bsql_cudf.ipynb)\n",
    "\n",
    "cuDF is a GPU DataFrame Library similar to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bc.sql('select * from taxi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some cudf function (like grabbing a column, or what have you...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell me about this taxi data\n",
    "bc.sql('select * from taxi').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about [BlazingSQL + cuDF](intro_notebooks/bsql_cudf.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Leverage your favorite Python visualization packages by converting a GPU DataFrame to a Pandas DataFrame with `.to_pandas()` or visualization packages that are GPU accelerated.\n",
    "\n",
    "### Non-GPU Visualization Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib/seaborn on a .to_pandas() gdf\n",
    "bc.sql('select count(*) from taxi group by day(date_time)').to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Visualization Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "from colorcet import fire\n",
    "\n",
    "# execute query & lay out a canvas w/ dropoff locations \n",
    "nyc = ds.Canvas().points(bc.sql('SELECT dropoff_x, dropoff_y FROM taxi'), 'dropoff_x', 'dropoff_y')\n",
    "\n",
    "# shade in the picture w/ fire & display\n",
    "ds.transfer_functions.set_background(ds.transfer_functions.shade(nyc, cmap=fire), \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "### cuML \n",
    "\n",
    "cuML is a GPU-accelerated machine learning library similar to scikit-learn but made to run on the GPU DataFrame.\n",
    "\n",
    "Let's predict fare amount of the `taxi` table we've been querying with a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from cuml import LinearRegression\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "\n",
    "# pull feature (X) and target (y) values\n",
    "X = bc.sql('SELECT trip_distance, tolls_amount FROM taxi')\n",
    "y = bc.sql('SELECT fare_amount FROM taxi')['fare_amount']\n",
    "\n",
    "# split data into train and test sets (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# call Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test X values\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# convert test & predicted values .to_pandas() & find the model's r2_score\n",
    "r2_score(y_true=y_test.to_pandas(), y_pred=y_pred.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Analytics\n",
    "\n",
    "### cuGraph - RAPIDS Graph Analytics Library\n",
    "\n",
    "Run graph analytics on GPU DataFrames with cuGraph, which aims to provide a NetworkX-like API on GPU DataFrames.\n",
    "\n",
    "Pending resolution of [rapidsai/cugraph#744](https://github.com/rapidsai/cugraph/issues/744)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cugraph\n",
    "\n",
    "# # assuming that data has been loaded into a cuDF (using read_csv) Dataframe\n",
    "# bc.create_table('karate', f'{os.getcwd()}/data/karate.csv', names=[\"src\", \"dst\"], delimiter='\\t', dtype=[\"float\", \"float\"])\n",
    "\n",
    "# # create a Graph using the source (src) and destination (dst) vertex pairs the GDF  \n",
    "# G = cugraph.Graph()\n",
    "# G.add_edge_list(gdf, source='src', destination='dst')  # ERROR\n",
    "\n",
    "# # Call cugraph.pagerank to get the pagerank scores\n",
    "# gdf_page = cugraph.pagerank(G)\n",
    "\n",
    "# for i in range(len(gdf_page)):\n",
    "#     print(\"vertex \" + str(gdf_page['vertex'][i]) + \" PageRank is \" + str(gdf_page['pagerank'][i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cuSignal - GPU-Accelerated Signal Processing\n",
    "\n",
    "cuSignal is a direct port of Scipy Signal built to leverage GPU compute resources through cuPy and Numba.\n",
    "\n",
    "<details><summary>...</summary>\n",
    "\n",
    "The RAPIDS cuSignal project leverages CuPy, Numba, and the RAPIDS ecosystem for GPU accelerated signal processing. \n",
    "    \n",
    "In some cases, cuSignal is a direct port of Scipy Signal to leverage GPU compute resources via CuPy but also contains Numba CUDA kernels for additional speedups for selected functions. \n",
    "    \n",
    "cuSignal achieves its best gains on large signals and compute intensive functions but stresses online processing with zero-copy memory (pinned, mapped) between CPU and GPU.\n",
    "\n",
    "[GitHub](https://github.com/rapidsai/cusignal) | [Intro Notebook](intro_notebooks/cusignal.ipynb)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cusignal\n",
    "import cupy as cp\n",
    "\n",
    "start = 0\n",
    "stop = 10\n",
    "num_samps = int(1e8)\n",
    "resample_up = 2\n",
    "resample_down = 3\n",
    "\n",
    "gx = cp.linspace(start, stop, num_samps, endpoint=False) \n",
    "gy = cp.cos(-gx**2/6.0)\n",
    "\n",
    "gf = cusignal.resample_poly(gy, resample_up, resample_down, window=('kaiser', 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Plugins - Scale Your Data\n",
    "\n",
    "<details><summary>...</summary>\n",
    "    \n",
    "We think you should let data rest wherever it likes. Don't worry about synching, directly query files wherever they reside.\n",
    "\n",
    "With the BlazingSQL Filesystem API, you can register and connect to multiple storage solutions. \n",
    "\n",
    "- [AWS](https://docs.blazingdb.com/docs/s3) \n",
    "- [Google Storage](https://docs.blazingdb.com/docs/google-cloud-storage)\n",
    "- [HDFS](https://docs.blazingdb.com/docs/hdfs)\n",
    "\n",
    "Once a filesystem is registered you can reference the user-defined file path when creating a new table off of a file.\n",
    "    \n",
    "[Docs](https://docs.blazingdb.com/docs/connecting-data-sources) | [Intro notebook](intro_notebooks/storage_plugins.ipynb)\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register AWS S3 storage bucket \n",
    "bc.s3('bsql_data', bucket_name='blazingsql-colab')\n",
    "\n",
    "# tag S3 {s3://} file path to specific data directory within 'bsql_data'\n",
    "tpch_sf10 = 's3://bsql_data/tpch_sf10/'\n",
    "\n",
    "# create 'orders' table from list of 10 orders files\n",
    "bc.create_table('orders', [f'{tpch_sf10}orders/0_0_{i}.parquet' for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BlazingSQL Logs\n",
    "\n",
    "<details><summary>...</summary>\n",
    "    \n",
    "BlazingSQL has an internal log that records events from every node from all queries run. The events include runtime query step execution information, performance timings, errors and warnings. \n",
    "\n",
    "The logs table is called `bsql_logs`. You can query the logs as if it were any other table, except you use the `.log()` function, instead of the `.sql()` function.\n",
    "    \n",
    "[Docs](https://docs.blazingdb.com/docs/blazingsql-logs) | [Intro Notebook](intro_notebooks/bsql_logs.ipynb)\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long did each successfully run query take?\n",
    "bc.log(\"SELECT log_time, query_id, duration FROM bsql_logs WHERE info = 'Query Execution Done' ORDER BY log_time DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cyber Log Accelerators \n",
    "\n",
    "RAPIDS Cyber Log Accelerators (CLX)\n",
    "\n",
    "<details><summary>...</summary>\n",
    "    \n",
    "CLX (\"clicks\") provides a collection of RAPIDS examples for security analysts, data scientists, and engineers to quickly get started applying RAPIDS and GPU acceleration to real-world cybersecurity use cases.\n",
    "\n",
    "The goal of CLX is to:\n",
    "\n",
    "- Allow cyber data scientists and SecOps teams to generate workflows, using cyber-specific GPU-accelerated primitives and methods, that let them interact with code using security language,\n",
    "- Make available pre-built use cases that demonstrate CLX and RAPIDS functionality that are ready to use in a Security Operations Center (SOC),\n",
    "- Accelerate log parsing in a flexible, non-regex method. and\n",
    "- Provide SIEM integration with GPU compute environments via RAPIDS and effectively extend the SIEM environment.\n",
    "    \n",
    "[GitHub](https://github.com/rapidsai/clx) | [Intro Notebook](intro_notebooks/clx.ipynb)\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import s3fs\n",
    "from os import path\n",
    "\n",
    "# download data\n",
    "if not path.exists(\"./splunk_faker_raw4\"):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(\"rapidsai-data/cyber/clx/splunk_faker_raw4\", \"./splunk_faker_raw4\")\n",
    "\n",
    "# read in alert data\n",
    "gdf = cudf.read_csv('./splunk_faker_raw4')\n",
    "gdf.columns = ['raw']\n",
    "\n",
    "# parse the alert data using CLX built-in parsers\n",
    "from clx.parsers.splunk_notable_parser import SplunkNotableParser\n",
    "\n",
    "snp = SplunkNotableParser()\n",
    "parsed_gdf = cudf.DataFrame()\n",
    "parsed_gdf = snp.parse(gdf, 'raw')\n",
    "\n",
    "# define function to round time to the day\n",
    "def round2day(epoch_time):\n",
    "    return int(epoch_time/86400)*86400\n",
    "\n",
    "# aggregate alerts by day\n",
    "parsed_gdf['time'] = parsed_gdf['time'].astype(int)\n",
    "parsed_gdf['day'] = parsed_gdf.time.applymap(round2day)\n",
    "day_rule_gdf = parsed_gdf[['search_name','day','time']].groupby(['search_name', 'day']).count().reset_index()\n",
    "day_rule_gdf.columns = ['rule', 'day', 'count']\n",
    "\n",
    "# import the rolling z-score function from CLX statistics\n",
    "from clx.analytics.stats import rzscore\n",
    "\n",
    "# pivot the alert data so each rule is a column\n",
    "def pivot_table(gdf, index_col, piv_col, v_col):\n",
    "    index_list = gdf[index_col].unique()\n",
    "    piv_gdf = cudf.DataFrame()\n",
    "    piv_gdf[index_col] = index_list\n",
    "    for group in gdf[piv_col].unique():\n",
    "        \n",
    "        temp_df = gdf[gdf[piv_col] == group]\n",
    "        temp_df = temp_df[[index_col, v_col]]\n",
    "        temp_df.columns = [index_col, group]\n",
    "        piv_gdf = piv_gdf.merge(temp_df, on=[index_col], how='left')\n",
    "        \n",
    "    piv_gdf = piv_gdf.set_index(index_col)\n",
    "    return piv_gdf.sort_index()\n",
    "\n",
    "alerts_per_day_piv = pivot_table(day_rule_gdf, 'day', 'rule', 'count').fillna(0)\n",
    "\n",
    "# create a new cuDF with the rolling z-score values calculated\n",
    "r_zscores = cudf.DataFrame()\n",
    "for rule in alerts_per_day_piv.columns:\n",
    "    x = alerts_per_day_piv[rule]\n",
    "    r_zscores[rule] = rzscore(x, 7) #7 day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_rule_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingSQL is built on top of the RAPIDS AI ecosystem. RAPIDS is based on the Apache Arrow columnar memory format, and cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.\n",
    "\n",
    "BlazingSQL is a SQL interface for cuDF, with various features to support large scale data science workflows and enterprise datasets.\n",
    "\n",
    "Query Data Stored Externally - a single line of code can register remote storage solutions, such as Amazon S3.\n",
    "Simple SQL - incredibly easy to use, run a SQL query and the results are GPU DataFrames (GDFs).\n",
    "Interoperable - GDFs are immediately accessible to any RAPIDS library for data science workloads.\n",
    "    \n",
    "BlazingContext is the Python API of BlazingSQL. \n",
    "    \n",
    "Initializing BlazingContext connects allows you to create tables, run queries and utilize the power of GPU accelerated SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Datashader\n",
    "\n",
    "Datashader is a data visualization library  Quickly and accurately render even the largest data.\n",
    "\n",
    "<details><summary>...</summary>\n",
    "    \n",
    "Datashader is a data rasterization pipeline for automating the process of creating meaningful representations of large amounts of data. Datashader breaks the creation of images of data into 3 main steps:\n",
    "\n",
    "1. Projection\n",
    "\n",
    "  - Each record is projected into zero or more bins of a nominal plotting grid shape, based on a specified glyph.\n",
    "\n",
    "2. Aggregation\n",
    "\n",
    "  - Reductions are computed for each bin, compressing the potentially large dataset into a much smaller aggregate array.\n",
    "\n",
    "3. Transformation\n",
    "\n",
    "  - These aggregates are then further processed, eventually creating an image.\n",
    "\n",
    "Using this very general pipeline, many interesting data visualizations can be created in a performant and scalable way. Datashader contains tools for easily creating these pipelines in a composable manner, using only a few lines of code. Datashader can be used on its own, but it is also designed to work as a pre-processing stage in a plotting library, allowing that library to work with much larger datasets than it would otherwise.\n",
    "    \n",
    "Datashader is part of the [HoloViz](https://github.com/holoviz) ecosystem for making browser-based data visualization in Python easier to use, easier to learn, and more powerful. See [holoviz.org](https://holoviz.org/) for related packages that you can use with Datashader and [status.holoviz.org](http://status.holoviz.org/) for the current status of each HoloViz project.\n",
    "\n",
    "Datashader is supported and maintained by [Anaconda](https://anaconda.com/).\n",
    "    \n",
    "[GitHub](https://github.com/holoviz/datashader/) | [Intro Notebook](intro_notebooks/cuml.ipynb)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "<details><summary>...</summary>\n",
    "\n",
    "cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.\n",
    "\n",
    "cuML enables data scientists, researchers, and software engineers to run traditional tabular ML tasks on GPUs without going into the details of CUDA programming. In most cases, cuML's Python API matches the API from scikit-learn.\n",
    "\n",
    "For large datasets, these GPU-based implementations can complete 10-50x faster than their CPU equivalents. For details on performance, see the cuML Benchmarks Notebook.\n",
    "    \n",
    "[GitHub](https://github.com/rapidsai/cuml) | [Intro Notebook](intro_notebooks/cuml.ipynb)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<details><summary>...</summary>\n",
    "\n",
    "The RAPIDS cuGraph library is a collection of graph analytics that process data found in GPU Dataframes - see cuDF.\n",
    "\n",
    "cuGraph aims to provide a NetworkX-like API that will be familiar to data scientists, so they can now build GPU-accelerated workflows more easily.\n",
    "    \n",
    "[GitHub](https://github.com/rapidsai/cugraph) | [Intro Notebook](intro_notebooks/cugraph.ipynb)\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPIDS Stable",
   "language": "python",
   "name": "rapids-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
